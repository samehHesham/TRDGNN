{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13677955,"sourceType":"datasetVersion","datasetId":8692180}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# E5: Build Heterogeneous Graph (HeteroData)\n\n**Milestone:** E5 - Heterogeneous Graph Construction  \n**Objective:** Build PyG HeteroData from Elliptic++ CSV files with temporal constraints\n\n**Node Types:**\n- Transaction: 203,769 nodes (93 local features)\n- Address: 823,942 nodes (52 features)\n\n**Edge Types:**\n- tx → tx: Transaction flows\n- addr → tx: Address inputs to transaction\n- tx → addr: Transaction outputs to address\n- addr → addr: Address-to-address connections\n\n**Output:**\n- `hetero_graph.pt`: PyG HeteroData object\n- `hetero_graph_summary.json`: Graph statistics\n- `node_mappings.json`: ID mappings","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"# Install dependencies\n!pip install -q torch torch-geometric pandas numpy tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:17:58.001077Z","iopub.execute_input":"2025-11-10T10:17:58.001891Z","iopub.status.idle":"2025-11-10T10:19:23.643036Z","shell.execute_reply.started":"2025-11-10T10:17:58.001864Z","shell.execute_reply":"2025-11-10T10:19:23.642117Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom pathlib import Path\nimport json\nfrom torch_geometric.data import HeteroData\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"GPU available: {torch.cuda.is_available()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:19:23.644796Z","iopub.execute_input":"2025-11-10T10:19:23.645147Z","iopub.status.idle":"2025-11-10T10:19:38.405234Z","shell.execute_reply.started":"2025-11-10T10:19:23.645121Z","shell.execute_reply":"2025-11-10T10:19:38.404532Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nGPU available: True\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"# Paths\nDATA_ROOT = Path('/kaggle/input/elliptic-dataset')  # Adjust to your Kaggle dataset path\nOUTPUT_DIR = Path('/kaggle/working')\n\n# Graph construction settings\nTOP_K_ADDRESSES = 100000  # Use top 100K most active addresses (MVP)\nUSE_ALL_ADDRESSES = False  # Set True to use all 823K addresses (requires ~12GB RAM)\n\n# Temporal splits (same as E3)\nTRAIN_FRAC = 0.6\nVAL_FRAC = 0.2\nTEST_FRAC = 0.2\n\nprint(f\"Data root: {DATA_ROOT}\")\nprint(f\"Output dir: {OUTPUT_DIR}\")\nprint(f\"Address strategy: {'All addresses' if USE_ALL_ADDRESSES else f'Top {TOP_K_ADDRESSES:,}'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:19:38.405919Z","iopub.execute_input":"2025-11-10T10:19:38.406357Z","iopub.status.idle":"2025-11-10T10:19:38.411571Z","shell.execute_reply.started":"2025-11-10T10:19:38.406337Z","shell.execute_reply":"2025-11-10T10:19:38.410651Z"}},"outputs":[{"name":"stdout","text":"Data root: /kaggle/input/elliptic-dataset\nOutput dir: /kaggle/working\nAddress strategy: Top 100,000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Load Transaction Nodes","metadata":{}},{"cell_type":"code","source":"print(\"Loading transaction nodes...\")\n\n# Load features and classes\ntx_features = pd.read_csv(DATA_ROOT / 'txs_features.csv')\ntx_classes = pd.read_csv(DATA_ROOT / 'txs_classes.csv')\n\n# Merge\ntx_data = tx_features.merge(tx_classes, on='txId', how='left')\n\nprint(f\"  Transactions loaded: {len(tx_data):,}\")\n\n# Create ID mappings\ntx_ids = tx_data['txId'].values\ntx_id_to_idx = {tx_id: idx for idx, tx_id in enumerate(tx_ids)}\ntx_idx_to_id = {idx: tx_id for idx, tx_id in enumerate(tx_ids)}\n\n# Extract LOCAL features only (AF1-AF93)\nlocal_features = [col for col in tx_data.columns if 'Local_feature' in col]\nprint(f\"  Features: {len(local_features)} (Local only)\")\n\n# Extract features\ntx_x = torch.FloatTensor(tx_data[local_features].values)\ntx_x = torch.nan_to_num(tx_x, nan=0.0, posinf=0.0, neginf=0.0)\n\n# Normalize\ntx_x = (tx_x - tx_x.mean(dim=0)) / (tx_x.std(dim=0) + 1e-8)\ntx_x = torch.nan_to_num(tx_x, nan=0.0)\n\n# Extract timestamps\ntx_timestamps = torch.LongTensor(tx_data['Time step'].values)\n\n# Extract labels (1=illicit→1, 2=licit→0, 3/NaN=unknown→-1)\ny_raw = tx_data['class'].fillna(3).astype(int).values\ntx_y = torch.LongTensor(np.where(y_raw == 1, 1, np.where(y_raw == 2, 0, -1)))\n\nprint(f\"  Labeled: {(tx_y >= 0).sum():,} / {len(tx_y):,}\")\nprint(f\"  Fraud: {(tx_y == 1).sum():,}, Legit: {(tx_y == 0).sum():,}\")\nprint(f\"  Feature shape: {tx_x.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:19:38.412427Z","iopub.execute_input":"2025-11-10T10:19:38.412725Z","iopub.status.idle":"2025-11-10T10:19:53.929784Z","shell.execute_reply.started":"2025-11-10T10:19:38.412707Z","shell.execute_reply":"2025-11-10T10:19:53.928972Z"}},"outputs":[{"name":"stdout","text":"Loading transaction nodes...\n  Transactions loaded: 203,769\n  Features: 93 (Local only)\n  Labeled: 46,564 / 203,769\n  Fraud: 4,545, Legit: 42,019\n  Feature shape: torch.Size([203769, 93])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Load Address Nodes","metadata":{}},{"cell_type":"code","source":"print(\"Loading address nodes...\")\n\n# Try combined file first (more efficient)\ncombined_file = DATA_ROOT / 'wallets_features_classes_combined.csv'\n\nif combined_file.exists():\n    print(\"  Using combined wallet file...\")\n    addr_data = pd.read_csv(combined_file)\nelse:\n    print(\"  Loading separate files...\")\n    addr_features = pd.read_csv(DATA_ROOT / 'wallets_features.csv')\n    addr_classes = pd.read_csv(DATA_ROOT / 'wallets_classes.csv')\n    addr_data = addr_features.merge(addr_classes, on='address', how='left')\n\nprint(f\"  Total addresses: {len(addr_data):,}\")\n\n# Select top K most active if not using all\nif not USE_ALL_ADDRESSES and TOP_K_ADDRESSES:\n    addr_data = addr_data.nlargest(TOP_K_ADDRESSES, 'total_txs')\n    print(f\"  Selected top {TOP_K_ADDRESSES:,} most active addresses\")\n\n# Create ID mappings\naddr_ids = addr_data['address'].values\naddr_id_to_idx = {addr_id: idx for idx, addr_id in enumerate(addr_ids)}\naddr_idx_to_id = {idx: addr_id for idx, addr_id in enumerate(addr_ids)}\n\n# Extract features (exclude ID, timestamp, class)\nfeature_cols = [col for col in addr_data.columns \n                if col not in ['address', 'Time step', 'class']]\nprint(f\"  Features: {len(feature_cols)}\")\n\n# Extract features\naddr_x = torch.FloatTensor(addr_data[feature_cols].values)\naddr_x = torch.nan_to_num(addr_x, nan=0.0, posinf=0.0, neginf=0.0)\n\n# Normalize\naddr_x = (addr_x - addr_x.mean(dim=0)) / (addr_x.std(dim=0) + 1e-8)\naddr_x = torch.nan_to_num(addr_x, nan=0.0)\n\n# Extract timestamps\naddr_timestamps = torch.LongTensor(addr_data['Time step'].values)\n\n# Extract labels\ny_raw = addr_data['class'].fillna(3).astype(int).values\naddr_y = torch.LongTensor(np.where(y_raw == 1, 1, np.where(y_raw == 2, 0, -1)))\n\nprint(f\"  Labeled: {(addr_y >= 0).sum():,} / {len(addr_y):,}\")\nprint(f\"  Fraud: {(addr_y == 1).sum():,}, Legit: {(addr_y == 0).sum():,}\")\nprint(f\"  Feature shape: {addr_x.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:19:53.931519Z","iopub.execute_input":"2025-11-10T10:19:53.931758Z","iopub.status.idle":"2025-11-10T10:20:07.025627Z","shell.execute_reply.started":"2025-11-10T10:19:53.931732Z","shell.execute_reply":"2025-11-10T10:20:07.024885Z"}},"outputs":[{"name":"stdout","text":"Loading address nodes...\n  Using combined wallet file...\n  Total addresses: 1,268,260\n  Selected top 100,000 most active addresses\n  Features: 55\n  Labeled: 31,754 / 100,000\n  Fraud: 3,880, Legit: 27,874\n  Feature shape: torch.Size([100000, 55])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Load Edges","metadata":{}},{"cell_type":"code","source":"def load_edges(edge_type, src_mapping, dst_mapping):\n    \"\"\"\n    Load edges for a specific type.\n    \n    Args:\n        edge_type: One of ['tx-tx', 'addr-tx', 'tx-addr', 'addr-addr']\n        src_mapping: Source node ID to index mapping\n        dst_mapping: Destination node ID to index mapping\n    \n    Returns:\n        edge_index: [2, E] tensor\n    \"\"\"\n    file_map = {\n        'tx-tx': 'txs_edgelist.csv',\n        'addr-tx': 'AddrTx_edgelist.csv',\n        'tx-addr': 'TxAddr_edgelist.csv',\n        'addr-addr': 'AddrAddr_edgelist.csv'\n    }\n    \n    print(f\"\\nLoading {edge_type} edges...\")\n    \n    edges_df = pd.read_csv(DATA_ROOT / file_map[edge_type])\n    cols = list(edges_df.columns)\n    src_col, dst_col = cols[0], cols[1]\n    \n    print(f\"  Total edges in file: {len(edges_df):,}\")\n    \n    # Filter to valid nodes\n    valid = (edges_df[src_col].isin(src_mapping) & \n             edges_df[dst_col].isin(dst_mapping))\n    \n    src_idx = edges_df.loc[valid, src_col].map(src_mapping).values\n    dst_idx = edges_df.loc[valid, dst_col].map(dst_mapping).values\n    \n    edge_index = torch.LongTensor(np.vstack([src_idx, dst_idx]))\n    \n    print(f\"  Valid edges: {edge_index.shape[1]:,}\")\n    \n    return edge_index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:20:07.026425Z","iopub.execute_input":"2025-11-10T10:20:07.026695Z","iopub.status.idle":"2025-11-10T10:20:07.032659Z","shell.execute_reply.started":"2025-11-10T10:20:07.026671Z","shell.execute_reply":"2025-11-10T10:20:07.031937Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load all edge types\nprint(\"=\"*70)\nprint(\"LOADING EDGES\")\nprint(\"=\"*70)\n\nedge_index_tx_tx = load_edges('tx-tx', tx_id_to_idx, tx_id_to_idx)\nedge_index_addr_tx = load_edges('addr-tx', addr_id_to_idx, tx_id_to_idx)\nedge_index_tx_addr = load_edges('tx-addr', tx_id_to_idx, addr_id_to_idx)\n\n# addr-addr edges may be very large - optional\ntry:\n    edge_index_addr_addr = load_edges('addr-addr', addr_id_to_idx, addr_id_to_idx)\nexcept Exception as e:\n    print(f\"\\nSkipping addr-addr edges: {e}\")\n    edge_index_addr_addr = torch.LongTensor([[],[]])  # Empty edges","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:20:07.033299Z","iopub.execute_input":"2025-11-10T10:20:07.033484Z","iopub.status.idle":"2025-11-10T10:20:14.391937Z","shell.execute_reply.started":"2025-11-10T10:20:07.033469Z","shell.execute_reply":"2025-11-10T10:20:14.391301Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nLOADING EDGES\n======================================================================\n\nLoading tx-tx edges...\n  Total edges in file: 234,355\n  Valid edges: 234,355\n\nLoading addr-tx edges...\n  Total edges in file: 477,117\n  Valid edges: 53,059\n\nLoading tx-addr edges...\n  Total edges in file: 837,124\n  Valid edges: 80,489\n\nLoading addr-addr edges...\n  Total edges in file: 2,868,964\n  Valid edges: 54,173\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Create Temporal Splits","metadata":{}},{"cell_type":"code","source":"print(\"\\nCreating temporal splits...\")\n\n# Sort timestamps and find boundaries\nsorted_times = np.sort(np.unique(tx_timestamps.numpy()))\nn_timesteps = len(sorted_times)\n\ntrain_end_idx = int(n_timesteps * TRAIN_FRAC)\nval_end_idx = int(n_timesteps * (TRAIN_FRAC + VAL_FRAC))\n\ntrain_time_end = sorted_times[train_end_idx - 1]\nval_time_end = sorted_times[val_end_idx - 1]\n\n# Create masks (only for labeled transactions)\nlabeled = tx_y >= 0\ntrain_mask = (tx_timestamps <= train_time_end) & labeled\nval_mask = ((tx_timestamps > train_time_end) & (tx_timestamps <= val_time_end)) & labeled\ntest_mask = (tx_timestamps > val_time_end) & labeled\n\nprint(f\"  Train: {train_mask.sum():,} (time <= {train_time_end})\")\nprint(f\"  Val:   {val_mask.sum():,} (time <= {val_time_end})\")\nprint(f\"  Test:  {test_mask.sum():,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:20:14.392601Z","iopub.execute_input":"2025-11-10T10:20:14.392818Z","iopub.status.idle":"2025-11-10T10:20:14.402804Z","shell.execute_reply.started":"2025-11-10T10:20:14.392799Z","shell.execute_reply":"2025-11-10T10:20:14.402008Z"}},"outputs":[{"name":"stdout","text":"\nCreating temporal splits...\n  Train: 26,381 (time <= 29)\n  Val:   8,999 (time <= 39)\n  Test:  11,184\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Build HeteroData","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"BUILDING HETERODATA\")\nprint(\"=\"*70)\n\n# Initialize HeteroData\ndata = HeteroData()\n\n# Add transaction nodes\ndata['transaction'].x = tx_x\ndata['transaction'].y = tx_y\ndata['transaction'].timestamp = tx_timestamps\ndata['transaction'].train_mask = train_mask\ndata['transaction'].val_mask = val_mask\ndata['transaction'].test_mask = test_mask\n\n# Add address nodes\ndata['address'].x = addr_x\ndata['address'].y = addr_y\ndata['address'].timestamp = addr_timestamps\n\n# Add edges\ndata['transaction', 'to', 'transaction'].edge_index = edge_index_tx_tx\ndata['address', 'to', 'transaction'].edge_index = edge_index_addr_tx\ndata['transaction', 'to', 'address'].edge_index = edge_index_tx_addr\ndata['address', 'to', 'address'].edge_index = edge_index_addr_addr\n\nprint(\"\\nHeteroData Summary:\")\nprint(data)\n\nprint(\"\\nNode Statistics:\")\nprint(f\"  Transactions: {data['transaction'].num_nodes:,}\")\nprint(f\"  Addresses: {data['address'].num_nodes:,}\")\n\nprint(\"\\nEdge Statistics:\")\nfor edge_type in data.edge_types:\n    src, rel, dst = edge_type\n    print(f\"  {src} -> {dst}: {data[edge_type].num_edges:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:20:14.403542Z","iopub.execute_input":"2025-11-10T10:20:14.403922Z","iopub.status.idle":"2025-11-10T10:20:14.429627Z","shell.execute_reply.started":"2025-11-10T10:20:14.403893Z","shell.execute_reply":"2025-11-10T10:20:14.429031Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nBUILDING HETERODATA\n======================================================================\n\nHeteroData Summary:\nHeteroData(\n  transaction={\n    x=[203769, 93],\n    y=[203769],\n    timestamp=[203769],\n    train_mask=[203769],\n    val_mask=[203769],\n    test_mask=[203769],\n  },\n  address={\n    x=[100000, 55],\n    y=[100000],\n    timestamp=[100000],\n  },\n  (transaction, to, transaction)={ edge_index=[2, 234355] },\n  (address, to, transaction)={ edge_index=[2, 53059] },\n  (transaction, to, address)={ edge_index=[2, 80489] },\n  (address, to, address)={ edge_index=[2, 54173] }\n)\n\nNode Statistics:\n  Transactions: 203,769\n  Addresses: 100,000\n\nEdge Statistics:\n  transaction -> transaction: 234,355\n  address -> transaction: 53,059\n  transaction -> address: 80,489\n  address -> address: 54,173\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Save Outputs","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"SAVING OUTPUTS\")\nprint(\"=\"*70)\n\n# Save HeteroData\ntorch.save(data, OUTPUT_DIR / 'hetero_graph.pt')\nprint(f\"\\nSaved HeteroData: {OUTPUT_DIR / 'hetero_graph.pt'}\")\n\n# Save summary\nsummary = {\n    'num_nodes': {\n        'transaction': data['transaction'].num_nodes,\n        'address': data['address'].num_nodes\n    },\n    'num_edges': {\n        f\"{src}_to_{dst}\": data[src, rel, dst].num_edges\n        for src, rel, dst in data.edge_types\n    },\n    'num_labeled': {\n        'transaction': (data['transaction'].y >= 0).sum().item(),\n        'address': (data['address'].y >= 0).sum().item()\n    },\n    'temporal_range': {\n        'transaction': [\n            data['transaction'].timestamp.min().item(),\n            data['transaction'].timestamp.max().item()\n        ],\n        'address': [\n            data['address'].timestamp.min().item(),\n            data['address'].timestamp.max().item()\n        ]\n    },\n    'feature_dims': {\n        'transaction': data['transaction'].x.shape[1],\n        'address': data['address'].x.shape[1]\n    },\n    'splits': {\n        'train': data['transaction'].train_mask.sum().item(),\n        'val': data['transaction'].val_mask.sum().item(),\n        'test': data['transaction'].test_mask.sum().item()\n    }\n}\n\nwith open(OUTPUT_DIR / 'hetero_graph_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\nprint(f\"Saved summary: {OUTPUT_DIR / 'hetero_graph_summary.json'}\")\n\n# Save node mappings (first 1000 of each for reference)\nmappings = {\n    'tx_id_to_idx_sample': {str(k): int(v) for k, v in list(tx_id_to_idx.items())[:1000]},\n    'addr_id_to_idx_sample': {str(k): int(v) for k, v in list(addr_id_to_idx.items())[:1000]}\n}\n\nwith open(OUTPUT_DIR / 'node_mappings_sample.json', 'w') as f:\n    json.dump(mappings, f, indent=2)\nprint(f\"Saved mappings: {OUTPUT_DIR / 'node_mappings_sample.json'}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"E5 MILESTONE COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\nNext: Download hetero_graph.pt for E6 (TRD-HHGTN training)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:20:14.430287Z","iopub.execute_input":"2025-11-10T10:20:14.430526Z","iopub.status.idle":"2025-11-10T10:20:14.589448Z","shell.execute_reply.started":"2025-11-10T10:20:14.430509Z","shell.execute_reply":"2025-11-10T10:20:14.588829Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nSAVING OUTPUTS\n======================================================================\n\nSaved HeteroData: /kaggle/working/hetero_graph.pt\nSaved summary: /kaggle/working/hetero_graph_summary.json\nSaved mappings: /kaggle/working/node_mappings_sample.json\n\n======================================================================\nE5 MILESTONE COMPLETE!\n======================================================================\n\nNext: Download hetero_graph.pt for E6 (TRD-HHGTN training)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Validation Checks","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*70)\nprint(\"VALIDATION CHECKS\")\nprint(\"=\"*70)\n\n# Check 1: Node counts match\nprint(\"\\n1. Node Counts:\")\nprint(f\"   Transactions: {data['transaction'].num_nodes:,} (expected: {len(tx_ids):,})\")\nprint(f\"   Addresses: {data['address'].num_nodes:,} (expected: {len(addr_ids):,})\")\nassert data['transaction'].num_nodes == len(tx_ids), \"Transaction count mismatch!\"\nassert data['address'].num_nodes == len(addr_ids), \"Address count mismatch!\"\nprint(\"   ✓ PASS\")\n\n# Check 2: Edge indices are valid\nprint(\"\\n2. Edge Index Validity:\")\nfor edge_type in data.edge_types:\n    src, rel, dst = edge_type\n    edge_index = data[edge_type].edge_index\n    if edge_index.shape[1] > 0:  # Only check non-empty edges\n        src_max = edge_index[0].max().item()\n        dst_max = edge_index[1].max().item()\n        src_nodes = data[src].num_nodes\n        dst_nodes = data[dst].num_nodes\n        print(f\"   {src} -> {dst}: src_max={src_max} < {src_nodes}, dst_max={dst_max} < {dst_nodes}\")\n        assert src_max < src_nodes, f\"{src} index out of bounds!\"\n        assert dst_max < dst_nodes, f\"{dst} index out of bounds!\"\nprint(\"   ✓ PASS\")\n\n# Check 3: Split sizes\nprint(\"\\n3. Split Integrity:\")\ntrain_count = data['transaction'].train_mask.sum().item()\nval_count = data['transaction'].val_mask.sum().item()\ntest_count = data['transaction'].test_mask.sum().item()\ntotal_labeled = (data['transaction'].y >= 0).sum().item()\nprint(f\"   Train: {train_count:,}\")\nprint(f\"   Val: {val_count:,}\")\nprint(f\"   Test: {test_count:,}\")\nprint(f\"   Total: {train_count + val_count + test_count:,} (labeled: {total_labeled:,})\")\nassert train_count + val_count + test_count == total_labeled, \"Split count mismatch!\"\nprint(\"   ✓ PASS\")\n\n# Check 4: No NaN in features\nprint(\"\\n4. Feature Quality:\")\ntx_nans = torch.isnan(data['transaction'].x).sum().item()\naddr_nans = torch.isnan(data['address'].x).sum().item()\nprint(f\"   Transaction NaNs: {tx_nans}\")\nprint(f\"   Address NaNs: {addr_nans}\")\nassert tx_nans == 0, \"Transaction features contain NaN!\"\nassert addr_nans == 0, \"Address features contain NaN!\"\nprint(\"   ✓ PASS\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ALL VALIDATION CHECKS PASSED!\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T10:20:14.590091Z","iopub.execute_input":"2025-11-10T10:20:14.590302Z","iopub.status.idle":"2025-11-10T10:20:14.722065Z","shell.execute_reply.started":"2025-11-10T10:20:14.590286Z","shell.execute_reply":"2025-11-10T10:20:14.721307Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nVALIDATION CHECKS\n======================================================================\n\n1. Node Counts:\n   Transactions: 203,769 (expected: 203,769)\n   Addresses: 100,000 (expected: 100,000)\n   ✓ PASS\n\n2. Edge Index Validity:\n   transaction -> transaction: src_max=203768 < 203769, dst_max=203766 < 203769\n   address -> transaction: src_max=99999 < 100000, dst_max=202785 < 203769\n   transaction -> address: src_max=202791 < 203769, dst_max=99999 < 100000\n   address -> address: src_max=99999 < 100000, dst_max=99987 < 100000\n   ✓ PASS\n\n3. Split Integrity:\n   Train: 26,381\n   Val: 8,999\n   Test: 11,184\n   Total: 46,564 (labeled: 46,564)\n   ✓ PASS\n\n4. Feature Quality:\n   Transaction NaNs: 0\n   Address NaNs: 0\n   ✓ PASS\n\n======================================================================\nALL VALIDATION CHECKS PASSED!\n======================================================================\n","output_type":"stream"}],"execution_count":11}]}